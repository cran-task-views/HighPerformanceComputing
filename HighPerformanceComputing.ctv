<!--Hey Emacs, make this -*- mode: XML -*- -->
<CRANTaskView>

   <name>HighPerformanceComputing</name>
   <topic>High-Performance and Parallel Computing with R</topic>
   <maintainer email="Dirk.Eddelbuettel@R-project.org">Dirk Eddelbuettel</maintainer>
   <version>2011-02-22</version>

  <info>
    <p>
      This CRAN task view contains a list of packages, grouped by topic, that
      are useful for high-performance computing (HPC) with R.  In this context, we
      are defining 'high-performance computing' rather loosely as just about anything
      related to pushing R a littler further: using compiled code,
      parallel computing (in both explicit and implicit modes), working with
      large objects as well as profiling. 
    </p>
    <p>
      Unless otherwise mentioned, all packages presented with hyperlinks 
      are available from CRAN, the
      Comprehensive R Archive Network. 
    </p>
    <p>
      Several of the areas discussed in this Task View are undergoing rapid
      change. Please send suggestions for additions and extensions for this task
      view to the <a href="mailto:Dirk.Eddelbuettel@R-project.org">task view maintainer</a>.
    </p>
    <p>
      Suggestions and corrections by Achim Zeileis, Markus Schmidberger,
      Martin Morgan, Max Kuhn, Tomas Radivoyevitch, Jochen Knaus, Tobias
      Verbeke, Hao Yu, David Rosenberg, Marco Enea, Ivo Welch and Jay Emerson
      are gratefully acknowledged.
    </p>

    <p>
      <strong>Parallel computing: Explicit parallelism</strong>
    </p>
    <ul>	     
      <li>Several packages provide the communications layer required for parallel
	computing. The first package in this area was
	<pkg>rpvm</pkg> by Li and Rossini which uses the PVM (Parallel
        Virtual Machine) standard and libraries. <pkg>rpvm</pkg> is no
        longer actively maintained.
      </li>
      <li>In recent years, the
        alternative MPI (Message Passing Interface) standard has become the
        de facto standard in parallel computing. It is supported in R via
  	the <pkg>Rmpi</pkg> by Yu. <pkg>Rmpi</pkg> package is mature yet actively
	maintained and offers access to numerous functions from the MPI
	API, as well as a number of R-specific extensions.  <pkg>Rmpi</pkg>
        can be used with the LAM/MPI, MPICH / MPICH2, Open MPI, and Deino MPI 
        implementations. It should be noted that LAM/MPI is now in
        maintenance mode, and new development is focussed on Open MPI.
      </li>
      <li>An alternative is provided by the <pkg>nws</pkg> (NetWorkSpaces)
        packages from REvolution Computing.  It is the successor to the
	earlier LindaSpaces approach to parallel computing, and is
	implemented on top of the Twisted networking toolkit for Python.
      </li>
      <li>The <pkg>snow</pkg> (Simple Network of Workstations) package by
        Tierney et al. can use PVM, MPI, NWS as well as direct networking
        sockets. It provides an abstraction layer by hiding the
	communications details. The <pkg>snowFT</pkg> package provides 
        fault-tolerance extensions to <pkg>snow</pkg>.
      </li>
      <li>The <pkg>snowfall</pkg> package by Knaus provides a more recent
	alternative to <pkg>snow</pkg>. Functions can be used in sequential or
	parallel mode.
      </li>
      <li>The <pkg>papply</pkg> package by Currie provided a subset of the
        <pkg>Rmpi</pkg> functionality, but is no longer actively maintained either.
      </li>
      <li>The <pkg>biopara</pkg> package by Lazar and Schoenfeld offers socket-based parallel
        execution with some support for load-balancing and fault-tolerance.
      </li>
      <li>The <pkg>taskPR</pkg> package by Samatova et al. builds on top of LAM/MPI and offers 
        parallel execution of tasks.
      </li>
      <li>The Simple Parallel R INTerface (SPRINT) package by Hill et al.
	(<a href="http://forge.nesc.ac.uk/projects/sprint/">link</a>, 
	<a href="ttp://www.biomedcentral.com/1471-2105/9/558">paper</a>)
	provides a prototype framework that allows the addition of
	parallelised functions to R for easy exploitation of HPC
	systems. Currently only a parallised correlation calculation is
	provided.
      </li>
      <li>The <pkg>foreach</pkg> package allows general iteration over
	elements in a collection without the use of an explicit loop
	counter. Using foreach without side effects also facilitates
	executing the loop in parallel which is possible via
	the <pkg>doMC</pkg> (using <pkg>multicore</pkg> on single
	workstations or <pkg>doSMP</pkg> on single workstations),
	<pkg>doSNOW</pkg> (using <pkg>snow</pkg>, see
	above), <pkg>doMPI</pkg> (using <pkg>Rmpi</pkg>) packages and
	<pkg>doRedis</pkg> (using <pkg>rredis</pkg>) packages.
      </li>
      <li>The <pkg>Rdsm</pkg> package provides a threads-like parallel
	computing environment, both on multicore machine and across the network
	by providing facilities inspired from distributed shared memory
	programming.
      </li>
    </ul>

    <p>
    <strong>Parallel computing: Implicit parallelism</strong>
    </p>
    <ul>	     
      <li>The pnmath package by Tierney 
	(<a href="http://www.stat.uiowa.edu/~luke/R/experimental/">link</a>)
	uses the Open MP parallel processing directives of recent compilers
        (such gcc 4.2 or later) for implicit parallelism by replacing a
        number of internal R functions with replacements that can make use of
        multiple cores --- without any explicit requests from the user.  The
        alternate pnmath0 package offers the same functionality using
        Pthreads for environments in which the newer compilers are not
        available.  Similar functionality is expected to become integrated
        into R 'eventually'.
      </li>
      <li>The romp package by Jamitzky was presented at useR! 2008
        (<a href="http://www.statistik.tu-dortmund.de/useR-2008/slides/Jamitzky.pdf">slides</a>)
        and offers another interface to Open MP using Fortran. The code is still
	pre-alpha and available from the Google Code project <gcode>romp</gcode>.
	An R-Forge project <rforge>romp</rforge> was initiated but there is no package, yet.
      </li>
      <li>The <pkg>fork</pkg> package by Warnes provides R-equivalents to low-level Unix system functions
        like fork, signal, wait, kill and exit in order to spawn
        sub-processes for parallel execution. 
      </li>
      <li>The <pkg>multicore</pkg> package by Urbanek provides a way of
        running parallel computations in R on machines with multiple cores or
        CPUs by making use of operating system
        facilities. The <pkg>doSMP</pkg> complements
        this using a shared memory parallelisation approach.
      </li>
      <li>The R/parallel package by Vera, Jansen and Suppi offers a C++-based master-slave dispatch
        mechanism for parallel execution (<a href="http://www.rparallel.org/">link</a>)
      </li>
      <li>
	The SPRINT package by Hill adds another parallel framework to R 
	(<a href="http://forge.nesc.ac.uk/projects/sprint">link</a>).
      </li>
      <li>
	The <pkg>mapReduce</pkg> package by Brown provides a simple framework
	for parallel computations following the Google mapReduce approach. It
	provides a pure R implementation,  a syntax following the mapReduce
	paper and a flexible and parallelizable back end. 
      </li>
    </ul>

    <p>
    <strong>Parallel computing: Grid computing</strong>
    </p>
    <ul>	     
      <li>The <pkg>GridR</pkg> package by Wegener et al. can be used in a grid computing
        environment via a web service, via ssh or via Condor or Globus.
      </li>
      <li>The multiR package by Grose was presented at useR! 2008 
	but has not been released. It may offer a snow-style framework on a grid computing platform.
      </li>
      <li>The <rforge>biocep-distrib</rforge> project by Chine offers a Java-based framework for local, Grid,
        or Cloud computing. It is under active development.
      </li>
      <li>
	The RHIPE package by Guha profides an interface between R and Hadoop
	for a Map/Reduce programming framework. (<a href="http://ml.stat.purdue.edu/rhipe">link</a>)
      </li>
    </ul>

    <p>
    <strong>Parallel computing: Random numbers</strong>
    </p>
    <ul>	     
      <li>Random-number generators for parallel computing are available via
        the <pkg>rsprng</pkg> package by Li, and the <pkg>rlecuyer</pkg>
        package by Sevcikova and Rossini.
      </li>
    </ul>

    <p>
    <strong>Parallel computing: Resource managers and batch schedulers</strong>
    </p>
    <ul>	     
       <li>
        Job-scheduling toolkits permit management of
	parallel computing resources and tasks.  The slurm (Simple Linux
        Utility for Resource Management) set of programs (written by a
        consortium led by Lawrence Livermore Labs) works well with
        MPI. (<a href="https://computing.llnl.gov/linux/slurm/">link</a>)
      </li>
      <li>
        The Condor toolkit (<a href="http://www.cs.wisc.edu/condor/">link</a>) from 
	the University of Wisconsin-Madison has been used with R as described
	in this <a href="http://www.r-project.org/doc/Rnews/Rnews_2005-2.pdf">R
        News article</a>.
      </li>
      <li>
        The sfCluster package by Knaus can be used with <pkg>snowfall</pkg>.
	(<a href="http://www.imbi.uni-freiburg.de/parallel/">link</a>) but is
	currently limited to LAM/MPI. 
      </li>
      <li>
        The <pkg>Rsge</pkg> package by Bode offers an interface to the Sun Grid
        Engine batch-queuing system.
      </li>
      <li>
        The <pkg>batch</pkg> package by Hoffmann can launch parallel computing
        requests onto a cluster and gather results.
      </li>
    </ul>

    <p>
    <strong>Parallel computing: Applications</strong>
    </p>
    <ul>	     
      <li>
        The <pkg>caret</pkg> package by Kuhn can use can use various frameworks
	(MPI, NWS etc) to parallelized cross-validation and bootstrap characterizations of predictive models.
      </li>
      <li>
	The <pkg>multtest</pkg> package by Pollard et al. can
	use <pkg>snow</pkg>, <pkg>Rmpi</pkg> or <pkg>rpvm</pkg> for
	resampling-based testing of multiple hypothesis.
      </li>
      <li>
	The <pkg>maanova</pkg> package by Wu can use <pkg>snow</pkg>
	and <pkg>Rmpi</pkg> for the analysis of micro-array experiments.
      </li>
      <li>
	The <pkg>pvclust</pkg> package by Suzuki and Shimodaira can use <pkg>snow</pkg>
	and <pkg>Rmpi</pkg> for hierarchical clustering via multiscale
	bootstraps; and the <pkg>scaleboot</pkg> package by Shimodaira can
	use <pkg>pvclust</pkg>, <pkg>snow</pkg> and <pkg>Rmpi</pkg> for
	computing approximately unbiased p-values via multiscale bootstraps.
      </li>
      <li>
	The <pkg>tm</pkg> package by Feinerer can use <pkg>snow</pkg>
	and <pkg>Rmpi</pkg> for parallelized text mining.
      </li>
      <li>
	The <pkg>varSelRF</pkg> package by Diaz-Uriarte can use <pkg>snow</pkg>
	and <pkg>Rmpi</pkg> for parallelized use of variable selection via
	random forests; and the <pkg>ADaCGH</pkg> package by Diaz-Uriarte and Rueda
	can use <pkg>Rmpi</pkg> and <pkg>papply</pkg> for parallelized
	analysis of array CGH data.
      </li>
      <li>
	The <pkg>bcp</pkg> package by Erdman and Emerson for the bayesian
	analysis of change points can use <pkg>nws</pkg> for parallelized operations.
      </li>
      <li>
	The <pkg>BARD</pkg> package by Altman for better automated redistring,
	the <pkg>GAMBoost</pkg> package by Binder for <code>glm</code>
	and <code>gam</code> model fitting via boosting using b-splines,
	the <pkg>Geneland</pkg> package by Estoup, Guillot and Santos for
	structure detection from multilocus genetic data,
	the <pkg>Matching</pkg> package by Sekhon for multivariate and propensity
	score matching,
	the <pkg>STAR</pkg> package by Pouzat for spike train analysis,
	the <pkg>bnlearn</pkg> package by Scutari for bayesian network
	structure learning,
	the <pkg>latentnet</pkg> package by Krivitsky and Handcock for latent
	position and cluster models,
	the <pkg>lga</pkg> package by Harrington for linear grouping analysis,
	the <pkg>peperr</pkg> package by Porzelius and Binder for parallised
	estimation of prediction error,
	the <pkg>orloca</pkg> package by Fernandez-Palacin and Munoz-Marquez
	for operations research locational analysis,
	the <pkg>rgenoud</pkg> package by Mebane and Sekhon for genetic
	optimization using derivatives 
	the <bioc>affyPara</bioc> package by Schmidberger, Vicedo and
	Mansmann for parallel normalization of Affymetrix microarrays, 
	the <bioc>puma</bioc> package by Pearson et al. which propagates
	uncertainty into standard microarray analyses such as differential
	expression
	and
	the <pkg>ccems</pkg> package for combinatorically complex equilibrium
	model selection
	all can use <pkg>snow</pkg> for parallelized operations using either
	one of the MPI, PVM, NWS or socket protocols supported by <pkg>snow</pkg>.
      </li>
      <li>The <gcode>bugsparallel</gcode> package uses <pkg>Rmpi</pkg> for distributed
          computing of multiple MCMC chains using WinBUGS. 
      </li>
      <li>The <pkg>partDSA</pkg> package uses <pkg>nws</pkg> for generating a
          piecewise constant estimation list of increasingly complex
          predictors based on an intensive and comprehensive search over the
          entire covariate space.
      </li>
      <li>The <pkg>dclone</pkg> package provides a global optimization
	approach and a variant of simulated annealing which exploits Bayesian
	MCMC tools to get MLE point estimates and standard errors using low
	level functions for implementing maximum likelihood estimating
	procedures for complex models using data cloning and Bayesian Markov
	chain Monte Carlo methods with support for JAGS, WinBUGS and
	OpenBUGS; parallel computing is supported via the <pkg>snow</pkg>
	package.
      </li>
    </ul>

    <p>
    <strong>Parallel computing: GPUs</strong>
    </p>
    <ul>	     
      <li>
        The <pkg>gputools</pkg> package by Buckner provides several common
        data-mining algorithms which are implemented using a mixture of
        nVidia's CUDA langauge and cublas library. Given a computer with
        an nVidia GPU these functions may be substantially more efficient
        than native R routines.  The <pkg>rpud</pkg> package provides an
        optimised distance metric for NVidia-based GPUs.
      </li>
      <li>
        The <pkg>cudaBayesreg</pkg> package by da Silva implements
        the <code>rhierLinearModel</code> from the <pkg>bayesm</pkg> package 
	using nVidia's CUDA langauge and tools to provide high-performance
        statistical analysis of fMRI voxels.
      </li>
      <li>
        The rgpu package (see below for link) aims to speed up bioinformatics
        analysis by using the GPU.
      </li>
      <li>
	The <pkg>magma</pkg> package provides an interface to the hybrid
	GPU/CPU library Magma (see below for link).
      </li>
      <li>
	The <pkg>gcbd</pkg> package implements a benchmarking framework for
	BLAS and GPUs (using <pkg>gputools</pkg>).
      </li>
    </ul>

    <p>
    <strong>Large memory and out-of-memory data</strong>
    </p>
    <ul>
      <li>The <pkg>biglm</pkg> package by Lumley uses incremental computations to
        offers <code>lm()</code> and <code>glm()</code> functionality to 
        data sets stored outside of R's main memory.
      </li>
      <li>The <pkg>ff</pkg> package by Adler et al. offers file-based access to data sets
        that are too large to be loaded into memory, along with a number of
        higher-level functions.
      </li>
      <li>The <pkg>bigmemory</pkg> package by Kane and Emerson permits storing large objects such
        as matrices in memory (as well as via files) and uses external
        pointer objects to refer to them.  This permits transparent access
        from R without bumping against R's internal memory limits.  Several R
        processes on the same computer can also shared big memory objects.
      </li>
      <li>A large number of database packages, and database-alike packages
        (such as <pkg>sqldf</pkg> by Grothendieck and <pkg>data.table</pkg>
        by Dowle) are also of potential interest but not reviewed here.
      </li>
      <li>The <pkg>HadoopStreaming</pkg> package provides a framework for
	writing map/reduce scripts for use in Hadoop Streaming; it also
	facilitates operating on data in a streaming fashion which does not
	require Hadoop.
      </li>
      <li>The <pkg>speedglm</pkg> package permits to fit (generalised) linear
	models to large data.  For in-memory data sets, speedlm() or
	speedglm() can be used along with update.speedlm() which can update
	fitted models with new data. For out-of-memory data sets, shglm() is
	available; it works in the presence of factors and can check for
	singular matrices.
      </li>
      <li>
	The <pkg>biglars</pkg> package by Seligman et al can use the <pkg>ff</pkg>
	to support large-than-memory datasets for least-angle regression,
	lasso and stepwise regression.
      </li>
    </ul>

    <p>
    <strong>Easier interfaces for Compiled code</strong>
    </p>
    <ul>
      <li>
	The <pkg>inline</pkg> package by Sklyar et al eases adding code in C,
	C++ or Fortran to R. It takes care of the compilation, linking and
	loading of embeded code segments that are stored as R strings.
      </li>
      <li>
	The <pkg>Rcpp</pkg> package by Eddelbuettel and Francois offers a
        number of C++ clases that makes transferring R objects to C++
        functions (and back) easier, and the <pkg>RInside</pkg> package
        by the same authors allows easy embedding of R itself into C++
        applications for faster and more direct data transfer.
      </li>
      <li>
	The <pkg>rJava</pkg> package by Urbanek provides a low-level
	interface to Java similar to the <code>.Call()</code> interface for C
	and C++.
      </li>
      <!-- <li>Fortran interfaces -->
      <!-- </li> -->
      <!-- <li> -->
      <!-- </li>Debugging tools -->
    </ul>

    <p>
    <strong>Profiling tools</strong>
    </p>
    <ul>
      <li>The <pkg>profr</pkg> package by Wickham can visualize output from
	the <code>Rprof</code> interface for profiling.
      </li>
      <li>The <pkg>proftools</pkg> package by Tierney can also be used to analyse profiling output.
      </li>
    </ul>

  </info>

  <packagelist>
    <pkg>ADaCGH</pkg>
    <pkg>BARD</pkg>
    <pkg>batch</pkg>
    <pkg>bcp</pkg>
    <pkg>biglars</pkg>
    <pkg>biglm</pkg>
    <pkg>bigmemory</pkg>
    <pkg>biopara</pkg>
    <pkg>bnlearn</pkg>
    <pkg>caret</pkg>
    <pkg>ccems</pkg>
    <pkg>cudaBayesreg</pkg>
    <pkg>dclone</pkg>
    <pkg>doMC</pkg>
    <pkg>doMPI</pkg>
    <pkg>doRedis</pkg>
    <pkg>doSMP</pkg>
    <pkg>doSNOW</pkg>
    <pkg>data.table</pkg>
    <pkg>ff</pkg>
    <pkg>foreach</pkg>
    <pkg>fork</pkg>
    <pkg>GAMBoost</pkg>
    <pkg>gcbd</pkg>
    <pkg>Geneland</pkg>
    <pkg>gputools</pkg>
    <pkg>GridR</pkg>
    <pkg>HadoopStreaming</pkg>
    <pkg>inline</pkg>
    <pkg>latentnet</pkg>
    <pkg>lga</pkg>
    <pkg>maanova</pkg>
    <pkg>magma</pkg>
    <pkg>mapReduce</pkg>
    <pkg>Matching</pkg>
    <pkg>multicore</pkg>
    <pkg>multtest</pkg>
    <pkg>nws</pkg>
    <pkg>orloca</pkg>
    <pkg>papply</pkg>
    <pkg>partDSA</pkg>
    <pkg>peperr</pkg>
    <pkg>profr</pkg>
    <pkg>proftools</pkg>
    <pkg>pvclust</pkg>
    <pkg>Rcpp</pkg>
    <pkg>Rdsm</pkg>
    <pkg>rgenoud</pkg>
    <pkg>RInside</pkg>
    <pkg>rJava</pkg>
    <pkg>rlecuyer</pkg>
    <pkg priority="core">Rmpi</pkg>
    <pkg>rpud</pkg>
    <pkg>rpvm</pkg>
    <pkg>Rsge</pkg>
    <pkg>rsprng</pkg>
    <pkg>scaleboot</pkg>
    <pkg priority="core">snow</pkg>
    <pkg>snowfall</pkg>
    <pkg>snowFT</pkg>
    <pkg>speedglm</pkg>
    <pkg>sqldf</pkg>
    <pkg>STAR</pkg>
    <pkg>taskPR</pkg>
    <pkg>tm</pkg>
    <pkg>varSelRF</pkg>
  </packagelist>

  <links>
    <a href="http://www.stat.uiowa.edu/~luke/classes/295-hpc/">HPC computing notes by Luke Tierney for HPC class at University of Iowa</a> 
    <a href="https://stat.ethz.ch/mailman/listinfo/r-sig-hpc/">Mailing List: R Special Interest Group High Performance Computing</a>
    <a href="http://www.jstatsoft.org/v31/i01/">Schmidberger, Morgan, Eddelbuettel, Yu, Tierney and Mansmann (2009) paper on 'State of the Art in Parallel Computing with R'</a>
    <a href="http://www.stat.uiowa.edu/~luke/R/experimental/">Luke Tierney's code directory for pnmath and pnmath0</a>
    <rforge>biocep-distrib</rforge>
    <bioc>affyPara</bioc>
    <bioc>puma</bioc>
    <gcode>romp</gcode>
    <gcode>bugsparallel</gcode>
    <a href="https://computing.llnl.gov/linux/slurm/">Slurm project at Lawrence Livermore National Laboratory</a>
    <a href="http://www.cs.wisc.edu/condor/">Condor project at University of Wisconsin-Madison</a>
    <a href="http://www.imbi.uni-freiburg.de/parallel/">Parallel Computing in R with sfCluster/snowfall</a>
    <a href="http://en.wikipedia.org/wiki/Message_Passing_Interface">Wikipedia: Message Passing Interface (MPI)</a>
    <a href="http://en.wikipedia.org/wiki/Parallel_Virtual_Machine">Wikipedia: Parallel Virtual Machine (PVM)</a>
    <a href="http://dirk.eddelbuettel.com/papers/ismNov2009introHPCwithR.pdf">Slides from Introduction to High-Performance Computing with R tutorial help in Nov 2009 at the Institute for Statistical Mathematics, Tokyo, Japan</a> 
    <a href="https://gforge.nbic.nl/projects/rgpu/">rgpu project at nbic.nl</a>
    <a href="http://icl.cs.utk.edu/magma/">Magma: Matrix Algebra on GPU and Multicore architectures</a>
  </links>

</CRANTaskView>

